# -*- coding: utf-8 -*-
"""Copy of Diet Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17msDEEMb3kKaZQot8INwIjKTmqoy8x9_

#Mounting Google Drive for File Access
"""

#Mount Google Drive to access files stored in it
#from google.colab import drive
#drive.mount('/content/drive')

"""#Importing Libraries for Data Processing and Model Building"""

# Import libraries for data handling, visualization, and machine learning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
from sklearn.preprocessing import StandardScaler

"""#Checking the missing values"""

df=pd.read_csv("Dataset.csv")
print("Missing values:\n", df.isnull().sum())
#Number of rows and columns
print("Number of rows and columns:\n", df.shape)
#Columns
print("Columns:\n", df.columns)

"""#Data Preprocessing, Model Training, and Evaluation"""

# Data preprocessing and feature selection
df = pd.read_csv("Dataset.csv")  # Load dataset
df = df.drop(["Unnamed: 0", "BMI_tags", "Label"], axis=1)  # Drop unnecessary columns
df = pd.get_dummies(df, columns=['gender'], drop_first=False)  # Convert categorical column 'gender' to dummy variables

# Define features and target variable
X = df.drop(['calories_to_maintain_weight'], axis=1)  # Features (excluding target variable)
Y = df['calories_to_maintain_weight']  # Target variable

scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)
X=pd.DataFrame(X_scaled,columns=X.columns)

# Print first few rows of features and target for verification
print("X:\n", X.head())
print("Y:\n", Y.head())

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)

# Train the model
modelLR = LinearRegression()
modelLR.fit(X_train, Y_train)

# Predict and evaluate model performance
Y_pred = modelLR.predict(X_test)
mse = mean_squared_error(Y_test, Y_pred)  # Calculate Mean Squared Error
print(f'Mean Squared Error: {mse}')

accuracy = modelLR.score(X_test, Y_test)  # Calculate model accuracy
print(f'Accuracy: {accuracy * 100:.2f}%')

mae=mean_absolute_error(Y_test, Y_pred)  # Calculate Mean Absolute Error
print(f'Mean Absolute Error: {mae}')

r2=r2_score(Y_test, Y_pred)  # Calculate R-squared
print(f'R-squared: {r2}')

# Get the model coefficients and corresponding feature names
coefficients = modelLR.coef_
features = X.columns

# Create a DataFrame to display feature importance based on coefficients
importance_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})

# Plot a horizontal bar chart for feature importance
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Coefficient'], color='skyblue')  # Plotting the bar chart
plt.xlabel('Coefficient Value')  # Label for X-axis
plt.ylabel('Features')  # Label for Y-axis
plt.title('Feature Importance in Predicting Caloric Maintenance')  # Title of the plot
plt.gca().invert_yaxis()  # Invert Y-axis for better readability
plt.show()  # Display the plot

'''import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# Data preprocessing and feature selection
df = pd.read_csv("/content/drive/MyDrive/FML PROJECT/no/Dataset.csv")  # Load dataset
df = df.drop(["Unnamed: 0", "BMI_tags", "Label","BMI","BMR"], axis=1)  # Drop unnecessary columns
df = pd.get_dummies(df, columns=['gender'], drop_first=False)  # Convert categorical column 'gender' to dummy variables

# Define features and target variable
X = df.drop(['calories_to_maintain_weight'], axis=1)  # Features (excluding target variable)
Y = df['calories_to_maintain_weight']  # Target variable

scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)
X=pd.DataFrame(X_scaled,columns=X.columns)

# Print first few rows of features and target for verification
print("X:\n", X.head())
print("Y:\n", Y.head())

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Train the model
modelLR = LinearRegression()
modelLR.fit(X_train, Y_train)

# Predict and evaluate model performance
Y_pred = modelLR.predict(X_test)
mse = mean_squared_error(Y_test, Y_pred)  # Calculate Mean Squared Error
print(f'Mean Squared Error: {mse}')

mae=mean_absolute_error(Y_test, Y_pred)  # Calculate Mean Absolute Error
print(f'Mean Absolute Error: {mae}')

r2=r2_score(Y_test, Y_pred)  # Calculate R-squared
print(f'R-squared: {r2}')

# Get the model coefficients and corresponding feature names
coefficients = modelLR.coef_
features = X.columns

# Create a DataFrame to display feature importance based on coefficients
importance_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})

# Plot a horizontal bar chart for feature importance
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Coefficient'], color='skyblue')  # Plotting the bar chart
plt.xlabel('Coefficient Value')  # Label for X-axis
plt.ylabel('Features')  # Label for Y-axis
plt.title('Feature Importance in Predicting Caloric Maintenance')  # Title of the plot
plt.gca().invert_yaxis()  # Invert Y-axis for better readability
plt.show()  # Display the plot

# ============================================
# Compare Linear, Ridge, and Lasso Regression
# ============================================

from sklearn.linear_model import Ridge, Lasso

# Initialize models
ridge = Ridge(alpha=1.0, random_state=42)
lasso = Lasso(alpha=0.1, random_state=42)  # smaller alpha for Lasso (more sensitive)

models = {
    "Linear Regression": modelLR,
    "Ridge Regression": ridge,
    "Lasso Regression": lasso
}

# Fit and evaluate each model
for name, model in models.items():
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)

    mse = mean_squared_error(Y_test, Y_pred)
    mae = mean_absolute_error(Y_test, Y_pred)
    r2 = r2_score(Y_test, Y_pred)

    print(f"\n----- {name} -----")
    print(f"Mean Squared Error: {mse:.2f}")
    print(f"Mean Absolute Error: {mae:.2f}")
    print(f"R-squared: {r2:.4f}")
    print(f"Accuracy: {r2 * 100:.2f}%")

# ============================================
# Optional: check coefficients for Ridge and Lasso
# ============================================

coef_df = pd.DataFrame({
    "Feature": X.columns,
    "Linear": modelLR.coef_,
    "Ridge": ridge.coef_,
    "Lasso": lasso.coef_
})

print("\nFeature Coefficients Comparison:")
print(coef_df)

# If you want to visualize
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
coef_df.set_index("Feature").plot(kind="bar", figsize=(12,6))
plt.title("Feature Coefficient Comparison (Linear vs Ridge vs Lasso)")
plt.ylabel("Coefficient Value")
plt.grid(True)
plt.show()

# Continue from your existing code setup (X_train, X_test, Y_train, Y_test already defined)
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Define models
models = {
    "Linear Regression": modelLR,
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=42),
    "SVR (RBF Kernel)": SVR(kernel='rbf'),
    "MLP Regressor": MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', max_iter=1000, random_state=42)
}

# Evaluate models
results = []

for name, model in models.items():
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)

    mse = mean_squared_error(Y_test, Y_pred)
    mae = mean_absolute_error(Y_test, Y_pred)
    r2 = r2_score(Y_test, Y_pred)

    results.append({
        "Model": name,
        "R2 Score": round(r2, 4),
        "MSE": round(mse, 2),
        "MAE": round(mae, 2)
    })

# Display results as a DataFrame
import pandas as pd
results_df = pd.DataFrame(results).sort_values(by="R2 Score", ascending=False).reset_index(drop=True)
print(results_df)

import matplotlib.pyplot as plt

plt.figure(figsize=(6,6))
plt.scatter(Y_test, models["MLP Regressor"].predict(X_test), alpha=0.6)
plt.xlabel("Actual Calories")
plt.ylabel("Predicted Calories")
plt.title("Predicted vs Actual (MLP Regressor)")
plt.show()

for name, model in [("MLP", models["MLP Regressor"]),
                    ("XGBoost", models["XGBoost"]),
                    ("Random Forest", models["Random Forest"]),
                    ("Decision Tree", models["Decision Tree"]),
                    ("Linear Regression", modelLR),
                    ("SVR", models["SVR (RBF Kernel)"]),
                    ("Ridge", ridge),
                    ("Lasso", lasso)
                    ]:
    r2_train = model.score(X_train, Y_train)
    r2_test = model.score(X_test, Y_test)
    print(f"{name} - Train R²: {r2_train:.4f}, Test R²: {r2_test:.4f}")

"""#Extracting and Visualizing Feature Importance"""

# Get the model coefficients and corresponding feature names
coefficients = modelLR.coef_
features = X.columns

# Create a DataFrame to display feature importance based on coefficients
importance_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})

# Plot a horizontal bar chart for feature importance
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Coefficient'], color='skyblue')  # Plotting the bar chart
plt.xlabel('Coefficient Value')  # Label for X-axis
plt.ylabel('Features')  # Label for Y-axis
plt.title('Feature Importance in Predicting Caloric Maintenance')  # Title of the plot
plt.gca().invert_yaxis()  # Invert Y-axis for better readability
plt.show()  # Display the plot

"""#Plotting Actual vs Predicted Calories for Model Evaluation"""

# Plotting the actual vs predicted values to assess model performance
plt.figure(figsize=(10, 6))
plt.scatter(Y_test, Y_pred, alpha=0.5)  # Scatter plot of actual vs predicted values
plt.xlabel('Actual Calories to Maintain Weight')  # Label for X-axis
plt.ylabel('Predicted Calories to Maintain Weight')  # Label for Y-axis
plt.title('Actual vs Predicted Calories to Maintain Weight')  # Plot title
plt.plot([min(Y_test), max(Y_test)], [min(Y_test), max(Y_test)], 'r--')  # Diagonal line for perfect prediction
plt.legend()  # Show legend
plt.show()  # Display the plot'''